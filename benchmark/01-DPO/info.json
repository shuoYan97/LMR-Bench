{
    "instance_id": 1,
    "paper_name": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model",
    "folder_name": "01-DPO",
    "paper_url": "https://arxiv.org/pdf/2305.18290",
    "year": 2023,
    "repo_url": "https://github.com/eric-mitchell/direct-preference-optimization",
    "repo_folder_name": "direct-preference-optimization",
    "implementations": [
        {
            "instruction": "Implement the preference_loss function in trainers.py based on the DPO loss mentioned in the paper and the code repository. You may ignore the following parameters: ipo, reference_free and label_smoothing.",
            "index": 1,
            "category": "Training Objectives & Optimization Techniques",
            "goal_file": "trainers.py",
            "goal_function": "preference_loss",
            "class_name": "",
            "golden_file": "golden_files/trainers_golden.py",
            "retrieval_context": [],
            "unit_test_file": "unit_test/unit_test_1.py"
        }
    ]
}
